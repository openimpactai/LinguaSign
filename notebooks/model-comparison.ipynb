{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison for Sign Language Recognition\n",
    "\n",
    "This notebook compares different model architectures for sign language recognition. We'll train and evaluate multiple models on the same dataset to determine which performs best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path for imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Standard libraries\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Data processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import project modules\n",
    "from datasets.data_loader import get_dataloader\n",
    "from models.cnn_lstm import CNNLSTM, LandmarkCNNLSTM\n",
    "from models.mediapipe_ml import MediaPipeML\n",
    "from models.transformer import TransformerModel\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_palette('viridis')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Define paths and parameters for the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATA_DIR = Path('../datasets/processed/wlasl')\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Training configuration\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "PATIENCE = 5  # Early stopping patience\n",
    "\n",
    "# Model configuration\n",
    "MODELS = {\n",
    "    'landmark_cnn_lstm': LandmarkCNNLSTM,\n",
    "    'mediapipe_ml': MediaPipeML,\n",
    "    'transformer': TransformerModel\n",
    "}\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Load the dataset and prepare dataloaders for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset exists\n",
    "if not DATA_DIR.exists():\n",
    "    print(f\"Dataset directory {DATA_DIR} does not exist. Please run preprocessing scripts first.\")\n",
    "else:\n",
    "    # Load metadata\n",
    "    metadata_path = DATA_DIR / 'metadata.json'\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"Number of classes: {metadata['num_classes']}\")\n",
    "    print(f\"Total processed videos: {metadata['processed_videos']}\")\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader = get_dataloader(\n",
    "        DATA_DIR, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        split='train', \n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "    \n",
    "    val_dataloader = get_dataloader(\n",
    "        DATA_DIR, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        split='val', \n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "    \n",
    "    test_dataloader = get_dataloader(\n",
    "        DATA_DIR, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        split='test', \n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "    \n",
    "    print(f\"Train dataset size: {len(train_dataloader.dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_dataloader.dataset)}\")\n",
    "    print(f\"Test dataset size: {len(test_dataloader.dataset)}\")\n",
    "    \n",
    "    # Get a sample batch\n",
    "    sample_batch = next(iter(train_dataloader))\n",
    "    input_dim = sample_batch['features'].shape[-1]\n",
    "    print(f\"Input dimension: {input_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions\n",
    "\n",
    "Define functions for training and evaluating models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        # Move data to device\n",
    "        features = batch['features'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * features.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validating\", leave=False):\n",
    "            # Move data to device\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * features.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Store predictions and labels for further analysis\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_loss = running_loss / total\n",
    "    val_acc = correct / total\n",
    "    \n",
    "    return val_loss, val_acc, all_preds, all_labels\n",
    "\n",
    "def train_model(model_name, model, dataloaders, criterion, optimizer, device, num_epochs=20, patience=5):\n",
    "    \"\"\"Train a model with early stopping.\"\"\"\n",
    "    train_dataloader, val_dataloader = dataloaders\n",
    "    \n",
    "    # Initialize variables for early stopping\n",
    "    best_val_acc = 0.0\n",
    "    best_model_wts = model.state_dict().copy()\n",
    "    best_epoch = 0\n",
    "    no_improve_epochs = 0\n",
    "    \n",
    "    # Initialize lists for plotting\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    # Training loop\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Train for one epoch\n",
    "        train_loss, train_acc = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc, _, _ = validate(model, val_dataloader, criterion, device)\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Save statistics for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Check if this is the best model so far\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = model.state_dict().copy()\n",
    "            best_epoch = epoch\n",
    "            no_improve_epochs = 0\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "        \n",
    "        # Check if we should stop early\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Calculate training time\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print(f\"Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f} at epoch {best_epoch+1}\")\n",
    "    \n",
    "    # Load the best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accs': val_accs,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'best_epoch': best_epoch,\n",
    "        'training_time': time_elapsed\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, test_dataloader, criterion, device):\n",
    "    \"\"\"Evaluate the model on the test set.\"\"\"\n",
    "    test_loss, test_acc, all_preds, all_labels = validate(model, test_dataloader, criterion, device)\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "    \n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models\n",
    "\n",
    "Train different model architectures and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have the necessary data\n",
    "if 'train_dataloader' in locals() and 'val_dataloader' in locals() and 'test_dataloader' in locals():\n",
    "    # Initialize results dictionary\n",
    "    results = {}\n",
    "    \n",
    "    # Get number of classes\n",
    "    num_classes = metadata['num_classes']\n",
    "    \n",
    "    # Try each model\n",
    "    for model_name in ['landmark_cnn_lstm', 'transformer']:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {model_name} model\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Initialize model\n",
    "        if model_name == 'landmark_cnn_lstm':\n",
    "            model = LandmarkCNNLSTM(input_dim=input_dim, num_classes=num_classes)\n",
    "        elif model_name == 'transformer':\n",
    "            model = TransformerModel(input_dim=input_dim, num_classes=num_classes)\n",
    "        else:\n",
    "            print(f\"Model {model_name} not implemented for this notebook\")\n",
    "            continue\n",
    "        \n",
    "        # Move model to device\n",
    "        model = model.to(DEVICE)\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # Train the model\n",
    "        trained_model, training_stats = train_model(\n",
    "            model_name,\n",
    "            model,\n",
    "            (train_dataloader, val_dataloader),\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            DEVICE,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            patience=PATIENCE\n",
    "        )\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        print(f\"\\nEvaluating {model_name} on test set:\")\n",
    "        test_loss, test_acc = evaluate_model(trained_model, test_dataloader, criterion, DEVICE)\n",
    "        \n",
    "        # Store results\n",
    "        results[model_name] = {\n",
    "            'model': trained_model,\n",
    "            'training_stats': training_stats,\n",
    "            'test_loss': test_loss,\n",
    "            'test_acc': test_acc\n",
    "        }\n",
    "else:\n",
    "    print(\"Dataset not loaded. Please run the dataset loading cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Training Curves\n",
    "\n",
    "Plot the training and validation curves for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have results\n",
    "if 'results' in locals() and results:\n",
    "    # Plot training and validation curves\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax = axes[0]\n",
    "    for model_name, result in results.items():\n",
    "        stats = result['training_stats']\n",
    "        epochs = range(1, len(stats['train_losses']) + 1)\n",
    "        ax.plot(epochs, stats['train_losses'], 'o-', label=f\"{model_name} (train)\")\n",
    "        ax.plot(epochs, stats['val_losses'], 's--', label=f\"{model_name} (val)\")\n",
    "    \n",
    "    ax.set_title('Loss Curves')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    ax = axes[1]\n",
    "    for model_name, result in results.items():\n",
    "        stats = result['training_stats']\n",
    "        epochs = range(1, len(stats['train_accs']) + 1)\n",
    "        ax.plot(epochs, stats['train_accs'], 'o-', label=f\"{model_name} (train)\")\n",
    "        ax.plot(epochs, stats['val_accs'], 's--', label=f\"{model_name} (val)\")\n",
    "    \n",
    "    ax.set_title('Accuracy Curves')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Model Performance\n",
    "\n",
    "Compare the performance of different models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have results\n",
    "if 'results' in locals() and results:\n",
    "    # Prepare data for plotting\n",
    "    model_names = list(results.keys())\n",
    "    test_accs = [results[name]['test_acc'] for name in model_names]\n",
    "    training_times = [results[name]['training_stats']['training_time'] / 60 for name in model_names]  # in minutes\n",
    "    best_val_accs = [results[name]['training_stats']['best_val_acc'] for name in model_names]\n",
    "    \n",
    "    # Create a DataFrame for easy visualization\n",
    "    df = pd.DataFrame({\n",
    "        'Model': model_names,\n",
    "        'Test Accuracy': test_accs,\n",
    "        'Best Validation Accuracy': best_val_accs,\n",
    "        'Training Time (min)': training_times\n",
    "    })\n",
    "    \n",
    "    # Print the table\n",
    "    print(\"Model Performance Comparison:\")\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Plot test accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Model', y='Test Accuracy', data=df)\n",
    "    plt.title('Test Accuracy by Model')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    for i, acc in enumerate(test_accs):\n",
    "        plt.text(i, acc + 0.02, f\"{acc:.4f}\", ha='center')\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training time\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Model', y='Training Time (min)', data=df)\n",
    "    plt.title('Training Time by Model')\n",
    "    plt.ylabel('Time (minutes)')\n",
    "    for i, time in enumerate(training_times):\n",
    "        plt.text(i, time + 0.2, f\"{time:.2f}\", ha='center')\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Best Model\n",
    "\n",
    "Save the best performing model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have results\n",
    "if 'results' in locals() and results:\n",
    "    # Find the best model based on test accuracy\n",
    "    best_model_name = max(results.keys(), key=lambda k: results[k]['test_acc'])\n",
    "    best_model = results[best_model_name]['model']\n",
    "    best_test_acc = results[best_model_name]['test_acc']\n",
    "    \n",
    "    print(f\"Best model: {best_model_name} with test accuracy: {best_test_acc:.4f}\")\n",
    "    \n",
    "    # Create directory for saving models\n",
    "    save_dir = Path('../models/checkpoints')\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save the model\n",
    "    model_path = save_dir / f\"{best_model_name}_acc{best_test_acc:.4f}.pth\"\n",
    "    \n",
    "    if hasattr(best_model, 'save_checkpoint'):\n",
    "        best_model.save_checkpoint(model_path)\n",
    "    else:\n",
    "        torch.save(best_model.state_dict(), model_path)\n",
    "    \n",
    "    print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we compared different model architectures for sign language recognition. We trained and evaluated multiple models on the same dataset and analyzed their performance in terms of accuracy and training time.\n",
    "\n",
    "Based on our experiments, we found that:\n",
    "\n",
    "1. The LandmarkCNN-LSTM model achieves good accuracy with reasonable training time\n",
    "2. The Transformer model provides competitive accuracy but requires more training time\n",
    "3. [Add other insights based on your results]\n",
    "\n",
    "For the LinguaSign project, we recommend using the [best model based on your results] as it provides the best balance of accuracy and efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
