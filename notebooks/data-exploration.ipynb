{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Language Dataset Exploration\n",
    "\n",
    "This notebook explores sign language datasets used in the LinguaSign project. We'll visualize the data, extract statistics, and gain insights into the structure of the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path for imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Standard libraries\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Data processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Computer vision libraries\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_palette('viridis')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Configuration\n",
    "\n",
    "Set up paths to the datasets we want to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset paths\n",
    "DATASETS_DIR = Path('../datasets/raw')\n",
    "WLASL_DIR = DATASETS_DIR / 'wlasl'\n",
    "PHOENIX_DIR = DATASETS_DIR / 'phoenix'\n",
    "\n",
    "# Check if datasets exist\n",
    "print(f\"WLASL dataset exists: {WLASL_DIR.exists()}\")\n",
    "print(f\"PHOENIX dataset exists: {PHOENIX_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the datasets don't exist, you might need to download them first. You can use the scripts in the `datasets/download_scripts` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring WLASL Dataset\n",
    "\n",
    "Let's explore the WLASL (Word-Level American Sign Language) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WLASL metadata\n",
    "wlasl_json_path = list(WLASL_DIR.glob('WLASL_*.json'))[0]\n",
    "with open(wlasl_json_path, 'r') as f:\n",
    "    wlasl_data = json.load(f)\n",
    "\n",
    "print(f\"Number of glosses: {len(wlasl_data)}\")\n",
    "\n",
    "# Count total videos\n",
    "total_videos = sum(len(item['instances']) for item in wlasl_data)\n",
    "print(f\"Total number of videos: {total_videos}\")\n",
    "\n",
    "# Get the top 10 glosses with the most videos\n",
    "gloss_counts = [(item['gloss'], len(item['instances'])) for item in wlasl_data]\n",
    "gloss_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "top_glosses = gloss_counts[:10]\n",
    "\n",
    "# Plot the top glosses\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=[gloss for gloss, _ in top_glosses], y=[count for _, count in top_glosses])\n",
    "plt.title('Top 10 Glosses by Number of Videos')\n",
    "plt.xlabel('Gloss')\n",
    "plt.ylabel('Number of Videos')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the distribution of videos per gloss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts of videos per gloss\n",
    "video_counts = [len(item['instances']) for item in wlasl_data]\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(video_counts, bins=50)\n",
    "plt.title('Distribution of Videos per Gloss')\n",
    "plt.xlabel('Number of Videos')\n",
    "plt.ylabel('Number of Glosses')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"Mean videos per gloss: {np.mean(video_counts):.2f}\")\n",
    "print(f\"Median videos per gloss: {np.median(video_counts)}\")\n",
    "print(f\"Min videos per gloss: {np.min(video_counts)}\")\n",
    "print(f\"Max videos per gloss: {np.max(video_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Sign Language Data\n",
    "\n",
    "Let's visualize some videos from the dataset to better understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe solutions\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def visualize_video(video_path, max_frames=10):\n",
    "    \"\"\"Visualize a sign language video with MediaPipe landmarks.\"\"\"\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Video properties:\")\n",
    "    print(f\"  - Dimensions: {frame_width}x{frame_height}\")\n",
    "    print(f\"  - FPS: {fps}\")\n",
    "    print(f\"  - Total frames: {frame_count}\")\n",
    "    \n",
    "    # Sample frames\n",
    "    frame_indices = np.linspace(0, frame_count - 1, max_frames, dtype=int)\n",
    "    frames = []\n",
    "    \n",
    "    # Initialize MediaPipe Holistic\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        for idx in frame_indices:\n",
    "            # Set frame position\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                continue\n",
    "            \n",
    "            # Convert BGR image to RGB\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Process image with MediaPipe\n",
    "            results = holistic.process(image_rgb)\n",
    "            \n",
    "            # Draw landmarks\n",
    "            annotated_image = image_rgb.copy()\n",
    "            \n",
    "            # Draw pose landmarks\n",
    "            if results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    annotated_image, \n",
    "                    results.pose_landmarks, \n",
    "                    mp_holistic.POSE_CONNECTIONS\n",
    "                )\n",
    "            \n",
    "            # Draw hand landmarks\n",
    "            if results.left_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    annotated_image, \n",
    "                    results.left_hand_landmarks, \n",
    "                    mp_holistic.HAND_CONNECTIONS\n",
    "                )\n",
    "            if results.right_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    annotated_image, \n",
    "                    results.right_hand_landmarks, \n",
    "                    mp_holistic.HAND_CONNECTIONS\n",
    "                )\n",
    "            \n",
    "            frames.append(annotated_image)\n",
    "    \n",
    "    # Release video capture\n",
    "    cap.release()\n",
    "    \n",
    "    # Plot frames\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, frame in enumerate(frames):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "        axes[i].imshow(frame)\n",
    "        axes[i].set_title(f\"Frame {frame_indices[i]}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a random video from the dataset\n",
    "random_gloss = random.choice(wlasl_data)\n",
    "random_instance = random.choice(random_gloss['instances'])\n",
    "video_id = random_instance['video_id']\n",
    "video_path = WLASL_DIR / 'videos' / f\"{video_id}.mp4\"\n",
    "\n",
    "print(f\"Selected gloss: {random_gloss['gloss']}\")\n",
    "print(f\"Video ID: {video_id}\")\n",
    "\n",
    "if video_path.exists():\n",
    "    # Visualize the video\n",
    "    visualize_video(str(video_path))\n",
    "else:\n",
    "    print(f\"Video file not found: {video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Processed Features\n",
    "\n",
    "Now let's look at the processed features that are used for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to processed data\n",
    "PROCESSED_DIR = Path('../datasets/processed/wlasl')\n",
    "LANDMARKS_DIR = PROCESSED_DIR / 'landmarks'\n",
    "\n",
    "# Check if processed data exists\n",
    "if not LANDMARKS_DIR.exists():\n",
    "    print(\"Processed landmarks directory not found. Please run preprocessing scripts first.\")\n",
    "else:\n",
    "    # Count landmark files\n",
    "    landmark_files = list(LANDMARKS_DIR.glob('*.pkl'))\n",
    "    print(f\"Number of landmark files: {len(landmark_files)}\")\n",
    "    \n",
    "    # Load a random landmark file\n",
    "    random_landmark_file = random.choice(landmark_files)\n",
    "    with open(random_landmark_file, 'rb') as f:\n",
    "        landmarks = pickle.load(f)\n",
    "    \n",
    "    print(f\"Selected landmark file: {random_landmark_file.name}\")\n",
    "    print(f\"Number of frames: {len(landmarks)}\")\n",
    "    \n",
    "    # Print the keys in the landmarks\n",
    "    print(f\"Keys in landmarks: {landmarks[0].keys()}\")\n",
    "    \n",
    "    # Show shapes of different landmarks\n",
    "    for key in landmarks[0].keys():\n",
    "        if landmarks[0][key] is not None:\n",
    "            print(f\"{key} shape: {landmarks[0][key].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Hand Landmarks\n",
    "\n",
    "Let's visualize the hand landmarks for a better understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hand_landmarks(landmarks, frame_idx=0):\n",
    "    \"\"\"Plot hand landmarks in 3D.\"\"\"\n",
    "    frame = landmarks[frame_idx]\n",
    "    \n",
    "    # Check if hand landmarks exist\n",
    "    if frame['left_hand'] is None and frame['right_hand'] is None:\n",
    "        print(\"No hand landmarks found in this frame.\")\n",
    "        return\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Plot left hand\n",
    "    if frame['left_hand'] is not None:\n",
    "        ax1 = fig.add_subplot(121, projection='3d')\n",
    "        x = frame['left_hand'][:, 0]\n",
    "        y = frame['left_hand'][:, 1]\n",
    "        z = frame['left_hand'][:, 2]\n",
    "        ax1.scatter(x, y, z, c=range(len(x)), cmap='viridis')\n",
    "        \n",
    "        # Connect landmarks according to MediaPipe hand connections\n",
    "        connections = mp_holistic.HAND_CONNECTIONS\n",
    "        for connection in connections:\n",
    "            start_idx = connection[0]\n",
    "            end_idx = connection[1]\n",
    "            ax1.plot([x[start_idx], x[end_idx]], \n",
    "                     [y[start_idx], y[end_idx]], \n",
    "                     [z[start_idx], z[end_idx]], 'k-')\n",
    "        \n",
    "        ax1.set_title('Left Hand')\n",
    "        ax1.set_xlabel('X')\n",
    "        ax1.set_ylabel('Y')\n",
    "        ax1.set_zlabel('Z')\n",
    "    \n",
    "    # Plot right hand\n",
    "    if frame['right_hand'] is not None:\n",
    "        ax2 = fig.add_subplot(122, projection='3d')\n",
    "        x = frame['right_hand'][:, 0]\n",
    "        y = frame['right_hand'][:, 1]\n",
    "        z = frame['right_hand'][:, 2]\n",
    "        ax2.scatter(x, y, z, c=range(len(x)), cmap='viridis')\n",
    "        \n",
    "        # Connect landmarks according to MediaPipe hand connections\n",
    "        connections = mp_holistic.HAND_CONNECTIONS\n",
    "        for connection in connections:\n",
    "            start_idx = connection[0]\n",
    "            end_idx = connection[1]\n",
    "            ax2.plot([x[start_idx], x[end_idx]], \n",
    "                     [y[start_idx], y[end_idx]], \n",
    "                     [z[start_idx], z[end_idx]], 'k-')\n",
    "        \n",
    "        ax2.set_title('Right Hand')\n",
    "        ax2.set_xlabel('X')\n",
    "        ax2.set_ylabel('Y')\n",
    "        ax2.set_zlabel('Z')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot hand landmarks for a random frame\n",
    "if 'landmarks' in locals():\n",
    "    # Get a frame index where both hands are visible if possible\n",
    "    valid_frames = [i for i, frame in enumerate(landmarks) \n",
    "                   if frame['left_hand'] is not None or frame['right_hand'] is not None]\n",
    "    \n",
    "    if valid_frames:\n",
    "        frame_idx = random.choice(valid_frames)\n",
    "        print(f\"Plotting hand landmarks for frame {frame_idx}\")\n",
    "        plot_hand_landmarks(landmarks, frame_idx)\n",
    "    else:\n",
    "        print(\"No frames with hand landmarks found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Statistics\n",
    "\n",
    "Let's compute some statistics about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If processed data exists, compute statistics\n",
    "if LANDMARKS_DIR.exists():\n",
    "    # Collect statistics from a sample of landmark files\n",
    "    sample_size = min(100, len(landmark_files))\n",
    "    sample_files = random.sample(landmark_files, sample_size)\n",
    "    \n",
    "    # Statistics to collect\n",
    "    frame_counts = []\n",
    "    left_hand_counts = []\n",
    "    right_hand_counts = []\n",
    "    both_hands_counts = []\n",
    "    no_hands_counts = []\n",
    "    \n",
    "    # Collect statistics\n",
    "    for file in tqdm(sample_files, desc=\"Computing statistics\"):\n",
    "        with open(file, 'rb') as f:\n",
    "            lm = pickle.load(f)\n",
    "        \n",
    "        # Count frames\n",
    "        frame_counts.append(len(lm))\n",
    "        \n",
    "        # Count hands\n",
    "        left_hand_count = sum(1 for frame in lm if frame['left_hand'] is not None)\n",
    "        right_hand_count = sum(1 for frame in lm if frame['right_hand'] is not None)\n",
    "        both_hands_count = sum(1 for frame in lm \n",
    "                              if frame['left_hand'] is not None and frame['right_hand'] is not None)\n",
    "        no_hands_count = sum(1 for frame in lm \n",
    "                            if frame['left_hand'] is None and frame['right_hand'] is None)\n",
    "        \n",
    "        left_hand_counts.append(left_hand_count / len(lm))\n",
    "        right_hand_counts.append(right_hand_count / len(lm))\n",
    "        both_hands_counts.append(both_hands_count / len(lm))\n",
    "        no_hands_counts.append(no_hands_count / len(lm))\n",
    "    \n",
    "    # Compute statistics\n",
    "    print(f\"Frame count statistics:\")\n",
    "    print(f\"  - Mean: {np.mean(frame_counts):.2f}\")\n",
    "    print(f\"  - Median: {np.median(frame_counts)}\")\n",
    "    print(f\"  - Min: {np.min(frame_counts)}\")\n",
    "    print(f\"  - Max: {np.max(frame_counts)}\")\n",
    "    \n",
    "    print(f\"\\nHand detection statistics (percentage of frames):\")\n",
    "    print(f\"  - Left hand: {np.mean(left_hand_counts) * 100:.2f}%\")\n",
    "    print(f\"  - Right hand: {np.mean(right_hand_counts) * 100:.2f}%\")\n",
    "    print(f\"  - Both hands: {np.mean(both_hands_counts) * 100:.2f}%\")\n",
    "    print(f\"  - No hands: {np.mean(no_hands_counts) * 100:.2f}%\")\n",
    "    \n",
    "    # Plot frame count distribution\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(frame_counts, bins=20)\n",
    "    plt.title('Distribution of Frame Counts')\n",
    "    plt.xlabel('Number of Frames')\n",
    "    plt.ylabel('Number of Videos')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored the WLASL dataset and visualized sign language data. We've gained insights into the structure of the dataset, the distribution of videos per gloss, and the characteristics of the processed features.\n",
    "\n",
    "Key findings:\n",
    "- The dataset contains a diverse set of sign language gestures\n",
    "- MediaPipe effectively extracts hand and body landmarks\n",
    "- There is variability in the number of frames per video\n",
    "- Hand detection is not perfect, with some frames missing hand landmarks\n",
    "\n",
    "These insights will help us design better models for sign language recognition and translation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
